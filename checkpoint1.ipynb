{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import re\n",
    "\n",
    "from scipy.stats import skew, chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "import nltk\n",
    "from nltk.corpus import words, brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download('brown')\n",
    "\n",
    "# Get set of English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file_name):\n",
    "    \"\"\"\n",
    "    Loading the dataset with .csv\n",
    "\n",
    "    Params:\n",
    "        csv_file_name (string): File name with .csv extension\n",
    "\n",
    "    Returns:\n",
    "        main_df (dataframe): The dataframe of dataset being loaded\n",
    "    \"\"\"\n",
    "    main_df = pd.read_csv(csv_file_name)\n",
    "    main_df.head()\n",
    "    main_df['tld'] = main_df['host'].str.split('.', n=1).str[1]\n",
    "    main_df['cctld'] = main_df['tld'].str.split('.', n=1).str[1].fillna('None')\n",
    "    main_df = main_df.rename(columns = {'domain': 'subdomain'})\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii_domain(col):\n",
    "    \"\"\"\n",
    "    Checking the ASCII text\n",
    "\n",
    "    Params:\n",
    "        col (string): The column name to be checked\n",
    "\n",
    "    Returns:\n",
    "        int (int): The int of boolean types, either 1 or 0\n",
    "    \"\"\"\n",
    "    ascii_pattern = re.compile(r'^[a-zA-Z0-9.-]+$')\n",
    "    return int(bool(ascii_pattern.match(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legitToNumber(isdga):\n",
    "    \"\"\"\n",
    "    Convert the type dga/legit from isdga col to numeric types\n",
    "\n",
    "    Params:\n",
    "        isdga (string): The column name isdga\n",
    "\n",
    "    Returns:\n",
    "        int (int): The int of boolean types, either 1 or 0\n",
    "    \"\"\"\n",
    "    if isdga == 'legit':\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OH_Encoding(main_df, colname):\n",
    "    \"\"\"\n",
    "    Encode the categorical data type to numbers\n",
    "\n",
    "    Params:\n",
    "        main_df (dataframe): The df having colname\n",
    "        colname (string): The column to be encoded\n",
    "\n",
    "    Returns:\n",
    "        main_df (dataframe) if encoded successfully\n",
    "        'Error' raised otherwise\n",
    "    \"\"\"\n",
    "    if main_df[colname].dtype == 'object':\n",
    "        OH_encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded = OH_encoder.fit_transform(np.array(main_df[colname]).reshape(-1, 1))\n",
    "        encoded_features = pd.DataFrame(encoded, columns=OH_encoder.get_feature_names_out([colname]))\n",
    "        main_df = main_df.join(encoded_features)\n",
    "        main_df.drop(columns=[colname], inplace=True)\n",
    "        return main_df\n",
    "    else:\n",
    "        return 'Error, column is numeric type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_legit_dga(main_df):\n",
    "    \"\"\"\n",
    "    Split the df to legit and dga\n",
    "\n",
    "    Params:\n",
    "        main_df (dataframe): The df to be split\n",
    "\n",
    "    Returns:\n",
    "        legit_df, dga_df (dataframe): df after being split\n",
    "    \"\"\"\n",
    "\n",
    "    legit_df = main_df[(main_df['isDGA']=='legit')].reset_index().drop(columns='index')\n",
    "    dga_df = main_df[(main_df['isDGA']!='legit')].reset_index().drop(columns='index')\n",
    "    return legit_df, dga_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segment(df, col):\n",
    "    \"\"\"\n",
    "    Segment the words in text of col\n",
    "\n",
    "    Params:\n",
    "        df (dataframe): The df having col\n",
    "        col (string): The object type column to be word segmented\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): df after being segmented\n",
    "    \"\"\"\n",
    "\n",
    "    df['word_segment'] = df[col].apply(segment)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_true_textblob(df, col):\n",
    "    \"\"\"\n",
    "    Count # of true English words with TextBlob\n",
    "\n",
    "    Params:\n",
    "        df (dataframe): The df having col\n",
    "        col (string): The object type column to be checked\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): df after being checked\n",
    "    \"\"\"\n",
    "\n",
    "    def is_correct(word):\n",
    "        return TextBlob(word).correct() == word\n",
    "    \n",
    "    df['validation_textblob'] = df[col].apply(lambda words: [word for word in words if is_correct(word)])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def count_true_nltk(df, col):\n",
    "    \"\"\"\n",
    "    Count # of true English words with NLTK\n",
    "\n",
    "    Params:\n",
    "        df (dataframe): The df having col\n",
    "        col (string): The object type column to be checked\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): df after being checked\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_English(word):\n",
    "        return word in english_words\n",
    "    df['validation_nltk'] = df[col].apply(lambda words: [word for word in words if is_English(word)])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_word(df, col):\n",
    "    \"\"\"\n",
    "    Turning words in a list into their base form\n",
    "\n",
    "    Params:\n",
    "        df (dataframe): The df having col\n",
    "        col (string): The object type column having word list to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): df after being lemmatized\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemm(lst_text):\n",
    "        \"\"\"\" \n",
    "        Params:\n",
    "            lst_text (list): the list of words in a col\n",
    "        Returns:\n",
    "            lst_lemm (list): the list of words after being lemmatized\n",
    "        \"\"\"\n",
    "        lst_lemm = {lemmatizer.lemmatize(word) for word in lst_text}\n",
    "        return list(lst_lemm)\n",
    "    \n",
    "    df[col] = df[col].apply(lemm)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_text_segment(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Finalizing the text segment column\n",
    "\n",
    "    Params:\n",
    "        df (dataframe): The df having col1 and col2\n",
    "        col1 (string): The object type column 1 having word list to be compared\n",
    "        col2 (string): The object type column 2 having word list to be compared\n",
    "\n",
    "    Returns:\n",
    "        df (dataframe): df after being finalized\n",
    "    \"\"\"\n",
    "\n",
    "    def get_longer_list(row):\n",
    "        return row[col1] if len(row[col1]) > len(row[col2]) else row[col2]\n",
    "    \n",
    "    df['text_segment_final'] = df.apply(get_longer_list, axis=1)\n",
    "    \n",
    "    df.drop(columns=[col1, col2], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(csv_file_name):\n",
    "    main_df = load_data(csv_file_name)\n",
    "\n",
    "    main_df['ascii'] = main_df['subdomain'].apply(is_ascii_domain)\n",
    "    main_df['subdomain_len'] = main_df['subdomain'].str.len()\n",
    "    main_df['host_len'] = main_df['host'].str.len()\n",
    "\n",
    "    # Vowel count\n",
    "    main_df['subdomain_vowel_count'] = main_df['subdomain'].str.lower().str.count(r'[aeoiu]')\n",
    "\n",
    "    # Consonant count\n",
    "    main_df['subdomain_consonant_count'] = main_df['subdomain_len'] - main_df['subdomain_vowel_count']\n",
    "\n",
    "    # Has Numeric - boolean to int type\n",
    "    main_df['has_num'] = main_df['subdomain'].str.contains(r'\\d').astype(int)\n",
    "\n",
    "    # To Number for isDGA col\n",
    "    main_df['isDGA_N'] = main_df['isDGA'].apply(legitToNumber)\n",
    "    main_df['digitCount'] = main_df['subdomain'].str.count(r'\\d')\n",
    "\n",
    "    # Check if subdomain starting with digit\n",
    "    main_df['startW/Digit'] = main_df['subdomain'].str.match(r'^\\d').astype(int)\n",
    "\n",
    "    # OH Encoding subclass\n",
    "    main_df = OH_Encoding(main_df, 'subclass')\n",
    "    main_df = OH_Encoding(main_df, 'tld')\n",
    "    main_df = OH_Encoding(main_df, 'cctld')\n",
    "\n",
    "    legit_df, dga_df = split_legit_dga(main_df)\n",
    "\n",
    "    legit_df = word_segment(legit_df, 'subdomain')\n",
    "    legit_df = count_true_textblob(legit_df, 'word_segment')\n",
    "    legit_df = count_true_nltk(legit_df, 'word_segment')\n",
    "    legit_df = lemmatizer_word(legit_df, 'validation_textblob')\n",
    "    legit_df = lemmatizer_word(legit_df, 'validation_nltk')\n",
    "    legit_df = finalize_text_segment(legit_df, 'validation_textblob', 'validation_nltk')\n",
    "\n",
    "    dga_df = word_segment(dga_df, 'subdomain')\n",
    "    dga_df = count_true_textblob(dga_df, 'word_segment')\n",
    "    dga_df = count_true_nltk(dga_df, 'word_segment')\n",
    "    dga_df = lemmatizer_word(dga_df, 'validation_textblob')\n",
    "    dga_df = lemmatizer_word(dga_df, 'validation_nltk')\n",
    "    dga_df = finalize_text_segment(dga_df, 'validation_textblob', 'validation_nltk')\n",
    "\n",
    "    return legit_df, dga_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_df, dga_df = process('dga_data_small.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
